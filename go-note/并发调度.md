# sched

go 1.14.5

**比较粗糙，还有很多需要整理，部分内容可能理解上有问题**

**golang 线程的parking和unparking**

**早期的golang是单线程调度多个gr的。
后来演进成为了nm模型。gr都挂在全局的队列，所有的线程去全局队列里面去获取gr。因为锁竞争严重，后来抽象出p来调度所有的gr。每个p上面获取gr的成本是很低的。如果当前p获取不到再去全局的队列里面获取。这样大大降低了锁竞争。 [设计文档](https://docs.google.com/document/d/1TTj4T2JO42uD5ID9e89oa0sLKhJYD0Y_kqxDv3I3XMw/edit)**


为了防止出现饥饿，多个待执行的gr挂在一个p上，golang引入了抢占式的调度。这个抢占式不是将正在执行的gr停止然后去执行高优先级的。而是在调度的时候进行的。

对此有三种被拒绝的方案：

- 集中所有的gr统一调度。这无疑是回退了。
- 对于每个ready的gr，如果有空闲的p。就唤醒一个线程，然后立刻执行它。这会出现这个gr可能不会立刻工作的情况。
- 对于每个ready的gr，并且有一个空闲的p，就唤醒一个线程，但是不会立刻执行它。这个和第二种情况很类似。第二种情况可能不会导致线程的挂起？这种情况会导致大量的线程 挂起/唤醒

我理解第一种方案就是通过优先级排序所有的gr。

而第二第三种方案都是尝试立刻其他p或者global的gr。

##### 目前的方案：

如果有一个ready的gr，并且当前没有自旋的线程，则唤醒一个线程。

如果当前线程在本地队列、全局队列、netpoll中都不能获得一个gr，则自旋。调度器会记录自旋线程数量

线程唤醒也被认为是自旋，这种情况是不会移交gr的。也就是线程唤醒开始是不能工作的。

自旋的线程在挂起之前，会在每个p上尝试寻找gr。

如果自旋的线程找到了gr则运行，没找到则挂起。

如果调度器记录的自旋线程数量>1的话，这时候即使有可以运行的gr，也不会唤醒线程。

相应的，如果最后一个自旋的线程取消自旋以后，还会唤醒一个自旋的线程。


##### 旋转分为两种：

- m获得了p，寻找一个可用的g
- m没有获得p，正在寻找一个可用的p

**我理解，这种方案就是在上述的第三种方案的改进。在挂起和唤醒之间增加一个自旋状态。这样保证不会出现频繁的挂起/唤醒。只要有闲置的p，就一定有一个等待工作的m**

**对应的表现形式就是：**

**每当有一个可运行的g（新建或者唤醒），都会尝试增加一个自旋的m，这样可以保证这个g可以运行。**

**同时每次m获取了一个可运行的g，都会再尝试增加一个自旋的m，因为当前的m已经在执行这个g了。这样保证此时有一个自旋的m可以找到可执行的g。**

**在尝试增加自旋m的时候如果发现已经有一个了，就不增加了。**


```go
// The main goroutine.
// 主go执行流程
func main() {
	g := getg()
	// Allow newproc to start new Ms.
	mainStarted = true

	// 	将主go锁定在主线程上，保证所有的init操作都是在主线程上完成
	lockOSThread()

	// 再次校验，是否在主线程上
	if g.m != &m0 {
		throw("runtime.main not on m0")
	}

	// must be before defer
	// 这个大概意思是？ init的时候不能有defer？
	// 现在支持递归初始化
	// 也可以初始化执行多个函数
	// 标记了状态
	doInit(&runtime_inittask) 
	if nanotime() == 0 {
		throw("nanotime returning zero")
	}

	// 后续也可能会有解锁
	needUnlock := true
	defer func() {
		if needUnlock {
			unlockOSThread()
		}
	}()

	// Record when the world started.
	runtimeInitTime = nanotime()

	// 打开gc
	gcenable()

	main_init_done = make(chan bool)
	
	// cgo 先忽略
	if iscgo {...}

	// make an indirect call, as the linker doesn't know the address of the main package when laying down the runtime
	// 执行所有用户程序中的init函数
	// 间接调用？
	fn := main_init
	fn()
	close(main_init_done)

	// 这个时候defer不会进行解锁
	needUnlock = false
	unlockOSThread()

	if isarchive || islibrary {
		// A program compiled with -buildmode=c-archive or c-shared
		// has a main, but it is not executed.
		return
	}
	
	// 运行main函数
	fn = main_main // make an indirect call, as the linker doesn't know the address of the main package when laying down the runtime
	fn()

	// Make racy client program work: if panicking on
	// another goroutine at the same time as main returns,
	// let the other goroutine finish printing the panic trace.
	// Once it does, it will exit. See issues 3934 and 20018.
	// 保证其他goroutine发生panic以后，可以打印完整panic信息
	if atomic.Load(&runningPanicDefers) != 0 {
		// Running deferred functions should not take long.
		for c := 0; c < 1000; c++ {
			if atomic.Load(&runningPanicDefers) == 0 {
				break
			}
			Gosched()
		}
	}
	if atomic.Load(&panicking) != 0 {
		gopark(nil, nil, waitReasonPanicWait, traceEvGoStop, 1)
	}

	exit(0)
}
```

```go
// 创建goroutine的时候会调这个函数
func newproc(siz int32, fn *funcval) {
	// 这个应该是参数的起点地址
	// 不同的位数也不一样
	argp := add(unsafe.Pointer(&fn), sys.PtrSize)
	
	// 获取当前的g
	gp := getg()
	
	// pc 寄存器
	pc := getcallerpc()
	
	// 在系统栈上运行。
	// 每个非 dead 状态的 G 均被关联到用户栈上，即用户 Go 代码执行的地方。用户栈初始大小很小（例如 2K），然后动态的增加或减少。
	// 每个 M 均对应一个关联的系统栈（也成为 M 的 g0 栈，因为其作为一个 stub G 来实现）。在 Unix 平台上，则称为信号栈（也称之为 M 的 gsignal 栈）。系统和信号栈无法扩展，但已经大到足够运行运行时和 cgo 代码（一个纯 Go 二进制文件有 8K；而 cgo 二进制文件则有系统分配）。
	// 运行时代码经常会临时通过 systemstack, mcall 或 asmcgocall 切换到系统栈以执行那些无法扩展用户栈、或切换用户 goroutine 的不可抢占式任务。运行在系统栈上的代码是隐式不可抢占的、且不会被垃圾回收检测。当运行在系统栈上时，当前用户栈没有被用于执行代码。
	// from：https://changkun.us/archives/2018/09/255/
	systemstack(func() {
		newproc1(fn, (*uint8)(argp), siz, gp, pc)
	})
}
```

```go
// 创建一个g，并放入等待运行队列
func newproc1(fn *funcval, argp *uint8, narg int32, callergp *g, callerpc uintptr) {
	_g_ := getg()

	// 禁用抢占
	acquirem() // disable preemption because it can be holding p in a local var	
	// ？
	siz := narg
	siz = (siz + 7) &^ 7
	if siz >= _StackMin-4*sys.RegSize-sys.RegSize {
		throw("newproc: function arguments too large for new goroutine")
	}

	// 获取p的地址
	_p_ := _g_.m.p.ptr()
	
	// 从p上获取一个空闲的g，已经执行完的g不会立刻释放，而是在p的空闲队列上
	newg := gfget(_p_)
	
	// 如果没有，则创建
	if newg == nil {
		// 为g分配一个最小的内存
		newg = malg(_StackMin)
		
		// 看名字是原子的更换g的状态
		casgstatus(newg, _Gidle, _Gdead)
		
		// 放入allg列表中
		// publishes with a g->status of Gdead so GC scanner doesn't look at uninitialized stack.
		// 大概意思是根据g的状态，就可以判断gc是否需要扫描栈
		// 如果是个dead的g应该就不需要扫描了
		allgadd(newg) 
	}
	
	// g的栈下界是否有值
	// g的栈是分配在堆上的，所以可以很大。
	// 在堆上维护一个stack
	if newg.stack.hi == 0 {
		throw("newproc1: newg missing stack")
	}
	
	// 新的g都是dead的状态？
	if readgstatus(newg) != _Gdead {
		throw("newproc1: new g is not Gdead")
	}

	// 初始化现场和寄存器信息 todo
	totalSize := 4*sys.RegSize + uintptr(siz) + sys.MinFrameSize // extra space in case of reads slightly beyond frame
	totalSize += -totalSize & (sys.SpAlign - 1)                  // align to spAlign
	sp := newg.stack.hi - totalSize
	spArg := sp
	if usesLR {
		// caller's LR
		*(*uintptr)(unsafe.Pointer(sp)) = 0
		prepGoExitFrame(sp)
		spArg += sys.MinFrameSize
	}
	if narg > 0 {
		memmove(unsafe.Pointer(spArg), unsafe.Pointer(argp), uintptr(narg))
		// This is a stack-to-stack copy. If write barriers
		// are enabled and the source stack is grey (the
		// destination is always black), then perform a
		// barrier copy. We do this *after* the memmove
		// because the destination stack may have garbage on
		// it.
		if writeBarrier.needed && !_g_.m.curg.gcscandone {
			f := findfunc(fn.fn)
			stkmap := (*stackmap)(funcdata(f, _FUNCDATA_ArgsPointerMaps))
			if stkmap.nbit > 0 {
				// We're in the prologue, so it's always stack map index 0.
				bv := stackmapdata(stkmap, 0)
				bulkBarrierBitmap(spArg, spArg, uintptr(bv.n)*sys.PtrSize, 0, bv.bytedata)
			}
		}
	}

	memclrNoHeapPointers(unsafe.Pointer(&newg.sched), unsafe.Sizeof(newg.sched))
	newg.sched.sp = sp
	newg.stktopsp = sp
	newg.sched.pc = funcPC(goexit) + sys.PCQuantum // +PCQuantum so that previous instruction is in same function
	newg.sched.g = guintptr(unsafe.Pointer(newg))
	gostartcallfn(&newg.sched, fn)
	newg.gopc = callerpc
	newg.ancestors = saveAncestors(callergp)
	newg.startpc = fn.fn
	if _g_.m.curg != nil {
		newg.labels = _g_.m.curg.labels
	}
	if isSystemGoroutine(newg, false) {
		atomic.Xadd(&sched.ngsys, +1)
	}
	newg.gcscanvalid = false
	casgstatus(newg, _Gdead, _Grunnable)

	//	自增的goid
	if _p_.goidcache == _p_.goidcacheend {
		// Sched.goidgen is the last allocated id,
		// this batch must be [sched.goidgen+1, sched.goidgen+GoidCacheBatch].
		// At startup sched.goidgen=0, so main goroutine receives goid=1.
		_p_.goidcache = atomic.Xadd64(&sched.goidgen, _GoidCacheBatch)
		_p_.goidcache -= _GoidCacheBatch - 1
		_p_.goidcacheend = _p_.goidcache + _GoidCacheBatch
	}
	newg.goid = int64(_p_.goidcache)
	_p_.goidcache++
	
	// 放入p队列中
	// 这里第三个参数传的是true，表示下一个要执行的就是新创建的这个g
	runqput(_p_, newg, true)

	// 这里对应上面唤醒/挂起流程
	// 如果有空闲的p不是0，并且自旋的线程是0，创建一个新的自旋的m
	if atomic.Load(&sched.npidle) != 0 && atomic.Load(&sched.nmspinning) == 0 && mainStarted {
		wakep()
	}
	releasem(_g_.m)
}

func wakep() {
	// be conservative about spinning threads
	// cas操作，是不是代表，最多只能有一个自旋的m
	if !atomic.Cas(&sched.nmspinning, 0, 1) {
		return
	}
	// 生成一个自旋的m
	// 这个m可能是闲置的，也可能是新创建的
	startm(nil, true)
}
```

```go
// 从给定的p上获得一个g
// 这个g可能是执行完的空闲g
// 可能是从全局的g队列中获取的
func gfget(_p_ *p) *g {
retry:
	// 当前p没有，全局的p有
	if _p_.gFree.empty() && (!sched.gFree.stack.empty() || !sched.gFree.noStack.empty()) {
		lock(&sched.gFree.lock)
		// 转移最多32个g到当前的p上
		for _p_.gFree.n < 32 {
			// Prefer Gs with stacks.
			gp := sched.gFree.stack.pop()
			if gp == nil {
				gp = sched.gFree.noStack.pop()
				if gp == nil {
					break
				}
			}
			sched.gFree.n--
			_p_.gFree.push(gp)
			_p_.gFree.n++
		}
		unlock(&sched.gFree.lock)
		goto retry
	}
	// 从当前的p上获取
	gp := _p_.gFree.pop()
	if gp == nil {
		return nil
	}
	// 不用原子操作，一个p只能有一个正在运行的g
	_p_.gFree.n--
	if gp.stack.lo == 0 {
		// Stack was deallocated in gfput. Allocate a new one.
		// 拿到的g栈已经被释放了，分配一个
		systemstack(func() {
			gp.stack = stackalloc(_FixedStack)
		})
		gp.stackguard0 = gp.stack.lo + _StackGuard
	} else {
		if raceenabled {
			racemalloc(unsafe.Pointer(gp.stack.lo), gp.stack.hi-gp.stack.lo)
		}
		if msanenabled {
			msanmalloc(unsafe.Pointer(gp.stack.lo), gp.stack.hi-gp.stack.lo)
		}
	}
	return gp
}
```

```go
func malg(stacksize int32) *g {
	// 创建一个新的g
	newg := new(g)
	if stacksize >= 0 {
		stacksize = round2(_StackSystem + stacksize)
		// 必须是主go去操作？
		// 所有的栈都被allg引用，gc的时候更方便？
		systemstack(func() {
			newg.stack = stackalloc(uint32(stacksize))
		})
		// ？？
		newg.stackguard0 = newg.stack.lo + _StackGuard
		newg.stackguard1 = ^uintptr(0)
	}
	return newg
}
```

```go
// 把g交给p
// 有多种情况
// 如果next是true，说明这个g是立刻要执行的，直接放在p上下个要执行的位置
// 这个位置可能已经有g了，给他放在等待执行的队列。
// 如果是false，直接放入等待执行的队列。
// 如果等待执行队列满了，转移到全局的队列
func runqput(_p_ *p, gp *g, next bool) {
	// 应该是把一半的next的g不立刻执行，为什么呢？
	if randomizeScheduler && next && fastrand()%2 == 0 {
		next = false
	}
	
	if next {
	retryNext:
		// 这个runnext应该是下个即将运行的g
		// 在m的部分应该会有结果
		oldnext := _p_.runnext
		if !_p_.runnext.cas(oldnext, guintptr(unsafe.Pointer(gp))) {
			goto retryNext
		}
		// runnext是空的
		if oldnext == 0 {
			return
		}
		// 把上一个即将运行的g拿出来
		gp = oldnext.ptr()
	}

retry:
	// load-acquire, synchronize with consumers
	// 运行g队列的头和尾
	h := atomic.LoadAcq(&_p_.runqhead) 	
	t := _p_.runqtail
	
	// 小于长度？可以放入当前p的队列中
	if t-h < uint32(len(_p_.runq)) {
		// 循环队列，放入
		_p_.runq[t%uint32(len(_p_.runq))].set(gp)
		// 放入以后，t自增就可以了，看起来t是无限增长的
		// 那么h是否也是呢？
		// store-release, makes the item available for consumption
		atomic.StoreRel(&_p_.runqtail, t+1)		return
	}
	// 如果已经超过了runq的长度，就执行这个slow方法
	if runqputslow(_p_, gp, h, t) {
		return
	}
	// the queue is not full, now the put above must succeed
	goto retry
}

```

```go
// slow方法，这个名字
// 所有的g都挂在到p上或者一个全局的队列上，
// p上最多只有256个g，全局队列上没有限制
// 当p上已经有256个g了，那就转移一半放到全局的队列上
func runqputslow(_p_ *p, gp *g, h, t uint32) bool {
	// 一半的g，runq是256，这个batch是129，为啥是129呢
	var batch [len(_p_.runq)/2 + 1]*g

	// First, grab a batch from local queue.
	// 从当前的这个p上拿一部分
	n := t - h
	n = n / 2
	
	// 异常，可以忽略
	if n != uint32(len(_p_.runq)/2) {
		throw("runqputslow: queue is not full")
	}
	
	// 从h到n依次取n个，放到batch里面
	for i := uint32(0); i < n; i++ {
		batch[i] = _p_.runq[(h+i)%uint32(len(_p_.runq))].ptr()
	}
	
	// h向前移动n位，刚才说了，h和t是队列的头和尾
	if !atomic.CasRel(&_p_.runqhead, h, h+n) { // cas-release, commits consume
		return false
	}
	
	// 把刚加的的g也放进去了，所以上面是129个啊
	// 这个g就是new的那个g。
	// 上面说了如果new的g要立刻执行，则替换当前将要立刻执行的g
	// 所以这个g有两种可能，一种是刚来的，一种的立刻执行坑位被抢了，换下来的
	// 那立刻执行的是不是就掉下来了呢？
	batch[n] = gp


	// Link the goroutines.
	// 把g都穿成链表
	// 在p上是一个定长数组，在全局队列上是个单链表，因为不确定长度
	for i := uint32(0); i < n; i++ {
		batch[i].schedlink.set(batch[i+1])
	}
	
	// 组合链表
	var q gQueue
	q.head.set(batch[0])
	q.tail.set(batch[n])

	// Now put the batch on global queue.
	// 因为这个所以是slow吗，这个函数名应该是global吧
	lock(&sched.lock)
	
	// 放入全局的队列
	globrunqputbatch(&q, int32(n+1))
	unlock(&sched.lock)
	return true
}
```

```go
// 从p上拿来了一半的g，放进全局g链表
func globrunqputbatch(batch *gQueue, n int32) {
	// 单链表操作，不细看了
	sched.runq.pushBackAll(*batch)
	sched.runqsize += n
	*batch = gQueue{}
}
```

```go
// 没有真的唤醒一个m，只是做了一些准备工作
func startm(_p_ *p, spinning bool) {
	lock(&sched.lock)
	// 如果p是nil，尝试获取一个空闲的p
	if _p_ == nil {
		_p_ = pidleget()
		// 没有空闲的p可以用
		if _p_ == nil {
			unlock(&sched.lock)
			if spinning {
				// The caller incremented nmspinning, but there are no idle Ps,
				// so it's okay to just undo the increment and give up.
				// startm之前增加cas 0 1，这时候再减一？
				// 还可能是其他的地方调用
				// 生成一个自旋的m，之前一定是对这个计数器+1了，这里减回去
				if int32(atomic.Xadd(&sched.nmspinning, -1)) < 0 {
					throw("startm: negative nmspinning")
				}
			}
			// 没有获取到，直接返回了
			return
		}
	}
	// 有空闲的p可以用
	// 从空闲队列里面获取一个m
	mp := mget()
	
	// 现在才解锁，所以之前都不需要原子操作
	unlock(&sched.lock)
	if mp == nil {
		var fn func()
		if spinning {
			// The caller incremented nmspinning, so set m.spinning in the new M.
			fn = mspinning
		}
		// 没有空闲的，创建一个新的，
		// 生成一个自旋的m
		// 因为有空闲的p，没有空闲的m
		// todo
		newm(fn, _p_)
		return
	}
	
	// 获取到空闲的m
	// 获取到的m一定不是自旋的，因为是从空闲列表获取的
	// 自旋的m是可以自行寻找gr的
	if mp.spinning {
		throw("startm: m is spinning")
	}
	if mp.nextp != 0 {
		throw("startm: m has p")
	}
	if spinning && !runqempty(_p_) {
		throw("startm: p has runnable gs")
	}
	// The caller incremented nmspinning, so set m.spinning in the new M.
	mp.spinning = spinning
	
	// 给m设置传入或者是刚获取的空闲的p
	mp.nextp.set(_p_)
	
	// 唤醒m todo，这个很多地方用到了
	notewakeup(&mp.park)
}
```

```go
// 从midle里面获取一个m
func mget() *m {
	mp := sched.midle.ptr()
	if mp != nil {
		sched.midle = mp.schedlink
		sched.nmidle--
	}
	return mp
}
```

```go
// 这个时候真正唤醒一个m
func notewakeup(n *note) {
	var v uintptr
	for {
		v = atomic.Loaduintptr(&n.key)
		if atomic.Casuintptr(&n.key, v, locked) {
			break
		}
	}

	// Successfully set waitm to locked.
	// What was it before?
	switch {
	case v == 0:
		// Nothing was waiting. Done.
	case v == locked:
		// Two notewakeups! Not allowed.
		throw("notewakeup - double wakeup")
	default:
		// Must be the waiting m. Wake it up.
		semawakeup((*m)(unsafe.Pointer(v)))
	}
}
```

```go
func mstart() {
	_g_ := getg()

	osStack := _g_.stack.lo == 0
	if osStack {
		// Initialize stack bounds from system stack.
		// Cgo may have left stack size in stack.hi.
		// minit may update the stack bounds.
		size := _g_.stack.hi
		if size == 0 {
			size = 8192 * sys.StackGuardMultiplier
		}
		_g_.stack.hi = uintptr(noescape(unsafe.Pointer(&size)))
		_g_.stack.lo = _g_.stack.hi - size + 1024
	}
	// Initialize stack guard so that we can start calling regular
	// Go code.
	_g_.stackguard0 = _g_.stack.lo + _StackGuard
	// This is the g0, so we can also call go:systemstack
	// functions, which check stackguard1.
	_g_.stackguard1 = _g_.stackguard0
	mstart1()

	// Exit this thread.
	switch GOOS {
	case "windows", "solaris", "illumos", "plan9", "darwin", "aix":
		// Windows, Solaris, illumos, Darwin, AIX and Plan 9 always system-allocate
		// the stack, but put it in _g_.stack before mstart,
		// so the logic above hasn't set osStack yet.
		osStack = true
	}
	mexit(osStack)
}
```

```go
func mstart1() {
	_g_ := getg()

	if _g_ != _g_.m.g0 {
		throw("bad runtime·mstart")
	}

	// Record the caller for use as the top of stack in mcall and
	// for terminating the thread.
	// We're never coming back to mstart1 after we call schedule,
	// so other calls can reuse the current frame.
	save(getcallerpc(), getcallersp())
	asminit()
	minit()

	// Install signal handlers; after minit so that minit can
	// prepare the thread to be able to handle the signals.
	if _g_.m == &m0 {
		mstartm0()
	}

	// 创建m的时候注册的start函数
	// 如果创建的是自旋的，则将状态改为自旋
	if fn := _g_.m.mstartfn; fn != nil {
		fn()
	}

	if _g_.m != &m0 {
		acquirep(_g_.m.nextp.ptr())
		_g_.m.nextp = 0
	}
	
	// 真正的并发调度
	schedule()
}
```

```go
// 找到一个可执行的gr，永远不会返回
func schedule() {
	_g_ := getg()

	if _g_.m.locks != 0 {
		throw("schedule: holding locks")
	}

	if _g_.m.lockedg != 0 {
		stoplockedm()
		execute(_g_.m.lockedg.ptr(), false) // Never returns.
	}

	// We should not schedule away from a g that is executing a cgo call,
	// since the cgo call is using the m's g0 stack.
	if _g_.m.incgo {
		throw("schedule: in cgo")
	}

top:
	pp := _g_.m.p.ptr()
	pp.preempt = false

	if sched.gcwaiting != 0 {
		gcstopm()
		goto top
	}
	if pp.runSafePointFn != 0 {
		runSafePointFn()
	}

	// Sanity check: if we are spinning, the run queue should be empty.
	// Check this before calling checkTimers, as that might call
	// goready to put a ready goroutine on the local run queue.
	if _g_.m.spinning && (pp.runnext != 0 || pp.runqhead != pp.runqtail) {
		throw("schedule: spinning with local work")
	}

	checkTimers(pp, 0)

	var gp *g
	var inheritTime bool

	// Normal goroutines will check for need to wakeP in ready,
	// but GCworkers and tracereaders will not, so the check must
	// be done here instead.
	tryWakeP := false
	if trace.enabled || trace.shutdown {
		gp = traceReader()
		if gp != nil {
			casgstatus(gp, _Gwaiting, _Grunnable)
			traceGoUnpark(gp, 0)
			tryWakeP = true
		}
	}
	if gp == nil && gcBlackenEnabled != 0 {
		gp = gcController.findRunnableGCWorker(_g_.m.p.ptr())
		tryWakeP = tryWakeP || gp != nil
	}
	if gp == nil {
		// 每过一段时间就要去全局获取，保证公平
		if _g_.m.p.ptr().schedtick%61 == 0 && sched.runqsize > 0 {
			lock(&sched.lock)
			gp = globrunqget(_g_.m.p.ptr(), 1)
			unlock(&sched.lock)
		}
	}
	if gp == nil {
		// 从当前的p获取
		gp, inheritTime = runqget(_g_.m.p.ptr())
		// We can see gp != nil here even if the M is spinning,
		// if checkTimers added a local goroutine via goready.
	}
	
	if gp == nil {
		// 千方百计的获取一个g
		// 本地队列，全局队列，其他的p，netpoll
		// 总之返回了就一定会获取到。
		gp, inheritTime = findrunnable() // blocks until work is available
	}

	// 这里获得了一个g，如果当前是自旋的
	// 会解除自旋的状态，并且再次创建一个自旋的m
	if _g_.m.spinning {
		resetspinning()
	}

	if sched.disable.user && !schedEnabled(gp) {
		// Scheduling of this goroutine is disabled. Put it on
		// the list of pending runnable goroutines for when we
		// re-enable user scheduling and look again.
		lock(&sched.lock)
		if schedEnabled(gp) {
			// Something re-enabled scheduling while we
			// were acquiring the lock.
			unlock(&sched.lock)
		} else {
			sched.disable.runnable.pushBack(gp)
			sched.disable.n++
			unlock(&sched.lock)
			goto top
		}
	}

	// If about to schedule a not-normal goroutine (a GCworker or tracereader),
	// wake a P if there is one.
	if tryWakeP { // GCworker or tracereader还会再创建一个自旋的m
		if atomic.Load(&sched.npidle) != 0 && atomic.Load(&sched.nmspinning) == 0 {
			wakep()
		}
	}
	// 这个g可能被绑定到了某个m上 
	// 这时候会唤醒指定的m，stop当前的m
	if gp.lockedm != 0 {
		// Hands off own p to the locked m,
		// then blocks waiting for a new p.
		startlockedm(gp)
		goto top
	}

	execute(gp, inheritTime)
}

func execute(gp *g, inheritTime bool) {
	_g_ := getg()

	// Assign gp.m before entering _Grunning so running Gs have an
	// M.
	_g_.m.curg = gp
	gp.m = _g_.m
	casgstatus(gp, _Grunnable, _Grunning)
	gp.waitsince = 0
	gp.preempt = false
	gp.stackguard0 = gp.stack.lo + _StackGuard
	if !inheritTime {
		_g_.m.p.ptr().schedtick++
	}

	// Check whether the profiler needs to be turned on or off.
	hz := sched.profilehz
	if _g_.m.profilehz != hz {
		setThreadCPUProfiler(hz)
	}

	if trace.enabled {
		// GoSysExit has to happen when we have a P, but before GoStart.
		// So we emit it here.
		if gp.syscallsp != 0 && gp.sysblocktraced {
			traceGoSysExit(gp.sysexitticks)
		}
		traceGoStart()
	}

	gogo(&gp.sched)
}

// gr结束以后会调用这个函数
func goexit1() {
	if raceenabled {
		racegoend()
	}
	if trace.enabled {
		traceGoEnd()
	}
	mcall(goexit0)
}

// goexit continuation on g0.
func goexit0(gp *g) {
	_g_ := getg()

	casgstatus(gp, _Grunning, _Gdead)
	if isSystemGoroutine(gp, false) {
		atomic.Xadd(&sched.ngsys, -1)
	}
	
	// reset一些数据
	gp.m = nil
	locked := gp.lockedm != 0
	gp.lockedm = 0
	_g_.m.lockedg = 0
	gp.preemptStop = false
	gp.paniconfault = false
	gp._defer = nil // should be true already but just in case.
	gp._panic = nil // non-nil for Goexit during panic. points at stack-allocated data.
	gp.writebuf = nil
	gp.waitreason = 0
	gp.param = nil
	gp.labels = nil
	gp.timer = nil

	if gcBlackenEnabled != 0 && gp.gcAssistBytes > 0 {
		// Flush assist credit to the global pool. This gives
		// better information to pacing if the application is
		// rapidly creating an exiting goroutines.
		scanCredit := int64(gcController.assistWorkPerByte * float64(gp.gcAssistBytes))
		atomic.Xaddint64(&gcController.bgScanCredit, scanCredit)
		gp.gcAssistBytes = 0
	}

	dropg()

	if GOARCH == "wasm" { // no threads yet on wasm
		gfput(_g_.m.p.ptr(), gp)
		schedule() // never returns
	}

	if _g_.m.lockedInt != 0 {
		print("invalid m->lockedInt = ", _g_.m.lockedInt, "\n")
		throw("internal lockOSThread error")
	}
	
	// 放回到gr池
	gfput(_g_.m.p.ptr(), gp)
	if locked {
		// The goroutine may have locked this thread because
		// it put it in an unusual kernel state. Kill it
		// rather than returning it to the thread pool.

		// Return to mstart, which will release the P and exit
		// the thread.
		if GOOS != "plan9" { // See golang.org/issue/22227.
			gogo(&_g_.m.g0.sched)
		} else {
			// Clear lockedExt on plan9 since we may end up re-using
			// this thread.
			_g_.m.lockedExt = 0
		}
	}
	
	// 重新进入循环调度
	schedule()
}
```

# sched

go 1.12.5

**比较粗糙，还有很多需要整理**

```go
// The main goroutine.
// 主go执行流程
func main() {
	g := getg()
	// Allow newproc to start new Ms.
	mainStarted = true

	// 	将主go锁定在主线程上，保证所有的init操作都是在主线程上完成
	lockOSThread()

	// 再次校验，是否在主线程上
	if g.m != &m0 {
		throw("runtime.main not on m0")
	}

	// must be before defer
	// 这个大概意思是？ init的时候不能有defer？
	runtime_init()
	if nanotime() == 0 {
		throw("nanotime returning zero")
	}

	// 防止异常退出的时候，没有解锁
	needUnlock := true
	defer func() {
		if needUnlock {
			unlockOSThread()
		}
	}()

	// Record when the world started.
	runtimeInitTime = nanotime()

	// 打开gc
	gcenable()

	main_init_done = make(chan bool)
	
	// cgo 先忽略
	if iscgo {...}

	// make an indirect call, as the linker doesn't know the address of the main package when laying down the runtime
	// 执行所有用户程序中的init函数
	// 间接调用？
	fn := main_init
	fn()
	close(main_init_done)

	// 这个时候defer不会进行解锁
	needUnlock = false
	unlockOSThread()

	if isarchive || islibrary {
		// A program compiled with -buildmode=c-archive or c-shared
		// has a main, but it is not executed.
		return
	}
	
	// 运行main函数
	fn = main_main // make an indirect call, as the linker doesn't know the address of the main package when laying down the runtime
	fn()

	// Make racy client program work: if panicking on
	// another goroutine at the same time as main returns,
	// let the other goroutine finish printing the panic trace.
	// Once it does, it will exit. See issues 3934 and 20018.
	// 保证其他goroutine发生panic以后，可以打印完整panic信息
	if atomic.Load(&runningPanicDefers) != 0 {
		// Running deferred functions should not take long.
		for c := 0; c < 1000; c++ {
			if atomic.Load(&runningPanicDefers) == 0 {
				break
			}
			Gosched()
		}
	}
	if atomic.Load(&panicking) != 0 {
		gopark(nil, nil, waitReasonPanicWait, traceEvGoStop, 1)
	}

	exit(0)
}
```

```go
// 创建goroutine的时候会调这个函数
func newproc(siz int32, fn *funcval) {
	// 这个应该是参数的起点地址
	// 不同的位数也不一样
	argp := add(unsafe.Pointer(&fn), sys.PtrSize)
	
	// 获取当前的g
	gp := getg()
	
	// ？
	pc := getcallerpc()
	
	// 只是为用g0的栈上创建g
	systemstack(func() {
		newproc1(fn, (*uint8)(argp), siz, gp, pc)
	})
}
```

```go
// 创建一个g，并放入等待运行队列
func newproc1(fn *funcval, argp *uint8, narg int32, callergp *g, callerpc uintptr) {
	_g_ := getg()

	// 禁用抢占
	_g_.m.locks++ 
	
	// ？
	siz := narg
	siz = (siz + 7) &^ 7
	if siz >= _StackMin-4*sys.RegSize-sys.RegSize {
		throw("newproc: function arguments too large for new goroutine")
	}

	// 获取p的地址
	_p_ := _g_.m.p.ptr()
	
	// 从p上获取一个空闲的g，已经执行完的g不会立刻释放，而是在p的空闲队列上
	newg := gfget(_p_)
	
	// 如果没有，则创建
	if newg == nil {
		// 为g分配一个最小的内存
		newg = malg(_StackMin)
		
		// 看名字是原子的更换g的状态
		casgstatus(newg, _Gidle, _Gdead)
		
		// 放入allg列表中
		// publishes with a g->status of Gdead so GC scanner doesn't look at uninitialized stack.
		// 大概意思是根据g的状态，就可以判断gc是否需要扫描栈
		// 如果是个dead的g应该就不需要扫描了
		allgadd(newg) 
	}
	
	// g的栈下界是否有值
	// g的栈是分配在堆上的，所以可以很大。
	// 在堆上维护一个stack
	if newg.stack.hi == 0 {
		throw("newproc1: newg missing stack")
	}
	
	// 新的g都是dead的状态？
	if readgstatus(newg) != _Gdead {
		throw("newproc1: new g is not Gdead")
	}

	// 初始化现场和寄存器信息 todo
	totalSize := 4*sys.RegSize + uintptr(siz) + sys.MinFrameSize // extra space in case of reads slightly beyond frame
	totalSize += -totalSize & (sys.SpAlign - 1)                  // align to spAlign
	sp := newg.stack.hi - totalSize
	spArg := sp
	if usesLR {
		// caller's LR
		*(*uintptr)(unsafe.Pointer(sp)) = 0
		prepGoExitFrame(sp)
		spArg += sys.MinFrameSize
	}
	if narg > 0 {
		memmove(unsafe.Pointer(spArg), unsafe.Pointer(argp), uintptr(narg))
		// This is a stack-to-stack copy. If write barriers
		// are enabled and the source stack is grey (the
		// destination is always black), then perform a
		// barrier copy. We do this *after* the memmove
		// because the destination stack may have garbage on
		// it.
		if writeBarrier.needed && !_g_.m.curg.gcscandone {
			f := findfunc(fn.fn)
			stkmap := (*stackmap)(funcdata(f, _FUNCDATA_ArgsPointerMaps))
			if stkmap.nbit > 0 {
				// We're in the prologue, so it's always stack map index 0.
				bv := stackmapdata(stkmap, 0)
				bulkBarrierBitmap(spArg, spArg, uintptr(bv.n)*sys.PtrSize, 0, bv.bytedata)
			}
		}
	}

	memclrNoHeapPointers(unsafe.Pointer(&newg.sched), unsafe.Sizeof(newg.sched))
	newg.sched.sp = sp
	newg.stktopsp = sp
	newg.sched.pc = funcPC(goexit) + sys.PCQuantum // +PCQuantum so that previous instruction is in same function
	newg.sched.g = guintptr(unsafe.Pointer(newg))
	gostartcallfn(&newg.sched, fn)
	newg.gopc = callerpc
	newg.ancestors = saveAncestors(callergp)
	newg.startpc = fn.fn
	if _g_.m.curg != nil {
		newg.labels = _g_.m.curg.labels
	}
	if isSystemGoroutine(newg, false) {
		atomic.Xadd(&sched.ngsys, +1)
	}
	newg.gcscanvalid = false
	casgstatus(newg, _Gdead, _Grunnable)

	//	自增的goid
	if _p_.goidcache == _p_.goidcacheend {
		// Sched.goidgen is the last allocated id,
		// this batch must be [sched.goidgen+1, sched.goidgen+GoidCacheBatch].
		// At startup sched.goidgen=0, so main goroutine receives goid=1.
		_p_.goidcache = atomic.Xadd64(&sched.goidgen, _GoidCacheBatch)
		_p_.goidcache -= _GoidCacheBatch - 1
		_p_.goidcacheend = _p_.goidcache + _GoidCacheBatch
	}
	newg.goid = int64(_p_.goidcache)
	_p_.goidcache++
	
	// 放入p队列中
	// 这里第三个参数传的是true
	runqput(_p_, newg, true)

	if atomic.Load(&sched.npidle) != 0 && atomic.Load(&sched.nmspinning) == 0 && mainStarted {
		// 唤醒p？
		wakep()
	}
	_g_.m.locks--
	if _g_.m.locks == 0 && _g_.preempt { // restore the preemption request in case we've cleared it in newstack
		_g_.stackguard0 = stackPreempt
	}
}
```

```go
// 从给定的p上获得一个g
// 这个g可能是执行完的空闲g
// 可能是从全局的g队列中获取的
func gfget(_p_ *p) *g {
retry:
	// 当前p没有，全局的p有
	if _p_.gFree.empty() && (!sched.gFree.stack.empty() || !sched.gFree.noStack.empty()) {
		lock(&sched.gFree.lock)
		// 转移最多32个g到当前的p上
		for _p_.gFree.n < 32 {
			// Prefer Gs with stacks.
			gp := sched.gFree.stack.pop()
			if gp == nil {
				gp = sched.gFree.noStack.pop()
				if gp == nil {
					break
				}
			}
			sched.gFree.n--
			_p_.gFree.push(gp)
			_p_.gFree.n++
		}
		unlock(&sched.gFree.lock)
		goto retry
	}
	// 从当前的p上获取
	gp := _p_.gFree.pop()
	if gp == nil {
		return nil
	}
	// 不用原子操作，一个p只能有一个正在运行的g
	_p_.gFree.n--
	if gp.stack.lo == 0 {
		// Stack was deallocated in gfput. Allocate a new one.
		// 拿到的g栈已经被释放了，分配一个
		systemstack(func() {
			gp.stack = stackalloc(_FixedStack)
		})
		gp.stackguard0 = gp.stack.lo + _StackGuard
	} else {
		if raceenabled {
			racemalloc(unsafe.Pointer(gp.stack.lo), gp.stack.hi-gp.stack.lo)
		}
		if msanenabled {
			msanmalloc(unsafe.Pointer(gp.stack.lo), gp.stack.hi-gp.stack.lo)
		}
	}
	return gp
}
```

```go
func malg(stacksize int32) *g {
	// 创建一个新的g
	newg := new(g)
	if stacksize >= 0 {
		stacksize = round2(_StackSystem + stacksize)
		// 必须是主go去操作？
		// 所有的栈都被allg引用，gc的时候更方便？
		systemstack(func() {
			newg.stack = stackalloc(uint32(stacksize))
		})
		// ？？
		newg.stackguard0 = newg.stack.lo + _StackGuard
		newg.stackguard1 = ^uintptr(0)
	}
	return newg
}
```

```go
// 把g交给p
// 有多种情况
// 如果next是true，说明这个g是立刻要执行的，直接放在p上下个要执行的位置
// 这个位置可能已经有g了，给他放在等待执行的队列。
// 如果是false，直接放入等待执行的队列。
// 如果等待执行队列满了，转移到全局的队列
func runqput(_p_ *p, gp *g, next bool) {
	
	if next {
	retryNext:
		// 这个runnext应该是下个即将运行的g
		// 在m的部分应该会有结果
		oldnext := _p_.runnext
		if !_p_.runnext.cas(oldnext, guintptr(unsafe.Pointer(gp))) {
			goto retryNext
		}
		// runnext是空的
		if oldnext == 0 {
			return
		}
		// 把上一个即将运行的g拿出来
		gp = oldnext.ptr()
	}

retry:
	// load-acquire, synchronize with consumers
	// 运行g队列的头和尾
	h := atomic.LoadAcq(&_p_.runqhead) 	
	t := _p_.runqtail
	
	// 小于长度？可以放入当前p的队列中
	if t-h < uint32(len(_p_.runq)) {
		// 循环队列，放入
		_p_.runq[t%uint32(len(_p_.runq))].set(gp)
		// 放入以后，t自增就可以了，看起来t是无限增长的
		// 那么h是否也是呢？
		// store-release, makes the item available for consumption
		atomic.StoreRel(&_p_.runqtail, t+1)		return
	}
	// 如果已经超过了runq的长度，就执行这个slow方法
	if runqputslow(_p_, gp, h, t) {
		return
	}
	// the queue is not full, now the put above must succeed
	goto retry
}

```

```go
// slow方法，这个名字
// 所有的g都挂在到p上或者一个全局的队列上，
// p上最多只有256个g，全局队列上没有限制
// 当p上已经有256个g了，那就转移一半放到全局的队列上
func runqputslow(_p_ *p, gp *g, h, t uint32) bool {
	// 一半的g，runq是256，这个batch是129，为啥是129呢
	var batch [len(_p_.runq)/2 + 1]*g

	// First, grab a batch from local queue.
	// 从当前的这个p上拿一部分
	n := t - h
	n = n / 2
	
	// 异常，可以忽略
	if n != uint32(len(_p_.runq)/2) {
		throw("runqputslow: queue is not full")
	}
	
	// 从h到n依次取n个，放到batch里面
	for i := uint32(0); i < n; i++ {
		batch[i] = _p_.runq[(h+i)%uint32(len(_p_.runq))].ptr()
	}
	
	// h向前移动n位，刚才说了，h和t是队列的头和尾
	if !atomic.CasRel(&_p_.runqhead, h, h+n) { // cas-release, commits consume
		return false
	}
	
	// 把刚加的的g也放进去了，所以上面是129个啊
	// 这个g就是new的那个g。
	// 上面说了如果new的g要立刻执行，则替换当前将要立刻执行的g
	// 所以这个g有两种可能，一种是刚来的，一种的立刻执行坑位被抢了，换下来的
	// 那立刻执行的是不是就掉下来了呢？
	batch[n] = gp


	// Link the goroutines.
	// 把g都穿成链表
	// 在p上是一个定长数组，在全局队列上是个单链表，因为不确定长度
	for i := uint32(0); i < n; i++ {
		batch[i].schedlink.set(batch[i+1])
	}
	
	// 组合链表
	var q gQueue
	q.head.set(batch[0])
	q.tail.set(batch[n])

	// Now put the batch on global queue.
	// 因为这个所以是slow吗，这个函数名应该是global吧
	lock(&sched.lock)
	
	// 放入全局的队列
	globrunqputbatch(&q, int32(n+1))
	unlock(&sched.lock)
	return true
}
```

```go
// 从p上拿来了一半的g，放进全局g链表
func globrunqputbatch(batch *gQueue, n int32) {
	// 单链表操作，不细看了
	sched.runq.pushBackAll(*batch)
	sched.runqsize += n
	*batch = gQueue{}
}
```

```go
// 唤醒
func wakep() {
	// be conservative about spinning threads
	// 大概意思就是，需要在 ：足够的线程数去利用cpu资源和停放过多的线程去节省cpu的资源之间进行平衡。我理解也就是具体保持有多少个工作线程。
	// 避免唤醒过多的m，看起来只允许一次
	if !atomic.Cas(&sched.nmspinning, 0, 1) {
		return
	}
	// 唤醒m执行
	startm(nil, true)
}
```

```go
// 没有真的唤醒一个m，只是做了一些准备工作
func startm(_p_ *p, spinning bool) {
	lock(&sched.lock)
	// 如果p是nil，尝试获取一个空闲的p
	if _p_ == nil {
		_p_ = pidleget()
		if _p_ == nil {
			unlock(&sched.lock)
			if spinning {
				// The caller incremented nmspinning, but there are no idle Ps,
				// so it's okay to just undo the increment and give up.
				// m没有被成功唤醒，刚才的cas操作减一？
				if int32(atomic.Xadd(&sched.nmspinning, -1)) < 0 {
					throw("startm: negative nmspinning")
				}
			}
			// 没有获取到，直接返回了
			return
		}
	}
	// 获取一个空闲的m
	mp := mget()
	
	// 现在才解锁，所以之前都不需要原子操作
	unlock(&sched.lock)
	if mp == nil {
		var fn func()
		if spinning {
			// The caller incremented nmspinning, so set m.spinning in the new M.
			fn = mspinning
		}
		// 没有空闲的，创建一个新的，
		newm(fn, _p_)
		return
	}
	
	// 获取到空闲的m
	if mp.spinning {
		throw("startm: m is spinning")
	}
	if mp.nextp != 0 {
		throw("startm: m has p")
	}
	if spinning && !runqempty(_p_) {
		throw("startm: p has runnable gs")
	}
	// The caller incremented nmspinning, so set m.spinning in the new M.
	mp.spinning = spinning
	
	// 给m设置传入或者是刚获取的空闲的p
	mp.nextp.set(_p_)
	
	// 唤醒m todo，这个很多地方用到了
	notewakeup(&mp.park)
}
```

